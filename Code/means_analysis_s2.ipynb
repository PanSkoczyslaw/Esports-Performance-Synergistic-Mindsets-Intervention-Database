{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4836d-9e40-4019-ae1c-299512d8172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moments - S3:\n",
    "# Baseline 1,2,3,4,5 minute (Marker name: 201), \n",
    "# Before the 1st match 1st minute (211), \n",
    "# Before the 1st match 2nd minute (212), \n",
    "# During the match 1st minute (213), \n",
    "# During the match 2nd minute (214), \n",
    "# After the 1st match 1st minute (215), \n",
    "# After the 1st match 2nd minute, (215)\n",
    "# and similarly for the 2nd to 8th matches -   \n",
    "# a total of 53 one-minute intervals from S3 (5 from the baseline and 6 from each match)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e39b1f-ae65-4f02-a865-349b55d4a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574df94d-4c5d-4318-b0d8-1ba13a81fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_channel(cols_template, channel):\n",
    "    '''\n",
    "    Function to modify a template of column names to adapt to each signal.\n",
    "\n",
    "    Input:\n",
    "    cols_template (list of str): A list of column names containing a placeholder \"[channel]\".\n",
    "    channel (str): The name of the channel to replace the placeholder.\n",
    "\n",
    "    Returns:\n",
    "    list of str: A list of column names with the \"[channel]\" placeholder replaced by the provided channel name.\n",
    "    '''\n",
    "    \n",
    "    return [re.sub(r\"\\[channel\\]\", channel, col) for col in cols_template]\n",
    "\n",
    "def process_hr_hrv_s2(file_path, bunch_of_cols, filename):\n",
    "    '''\n",
    "    Function to process heart rate (HR) and heart rate variability (HRV) data from ECG signals in a CSV file.\n",
    "\n",
    "    Input:\n",
    "    file_path (str): The path to the CSV file containing the ECG data.\n",
    "    bunch_of_cols (list of lists of str): A list of lists where each sublist contains column names for HR and HRV data.\n",
    "    filename (str): The name of the file to be used as the index for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with processed HR and HRV data. The index is based on the provided filename,\n",
    "                  and the columns are defined by the input `bunch_of_cols`.\n",
    "    '''\n",
    "    i = 0\n",
    "    bn_col = 0\n",
    "    df_sub = pd.read_csv(file_path)\n",
    "    markers = [201,211,212,213,214,215,216,221,222,223,224,225,226,231,232,233,234,235,236,241,242,243,244,245,246,251,252,253,254,255,256,261,262,263,264,265,266,271,272,273,274,275,276,281,282,\n",
    "               283,284,285,286] #procedure markers, needed to mark the start of the average counting moment for each interval.\n",
    "                                #for marker \"201\" there are 5 columns from each set with the label \"baseline\". For the next markers there is a single column.\n",
    "    dataframes = []\n",
    "    hr_hrv_cols = [col for sublist in bunch_of_cols[:2] for col in sublist]\n",
    "    df_hr = pd.DataFrame(columns=hr_hrv_cols)\n",
    "    df_hr.loc[0] = [None] * len(hr_hrv_cols)\n",
    "    \n",
    "    if bn_col == 0:\n",
    "        for marker in markers:\n",
    "            curr_idx = df_sub.index[df_sub['marker'] == marker] #determining the first index for the given marker in df\n",
    "            \n",
    "            if marker == 201:\n",
    "                \n",
    "                for y in range(0, 300000, 60000): #calculations initiated from a 5-minute baseline, with minute intervals\n",
    "                    ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]+y:curr_idx[0]+y+60000], sampling_rate=1000) #calculating minute intervals hr from the first found index based on the marker, with a sampling rate of 1000hz\n",
    "                    hr = nk.ecg_intervalrelated(ecg_signals).iloc[0, 0] \n",
    "                    df_hr.iloc[0, i] = hr #saving calculations in local df\n",
    "                    i += 1 #moving to the next column\n",
    "            else:\n",
    "                ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]-1:curr_idx[0]+60000], sampling_rate=1000)\n",
    "                hr = nk.ecg_intervalrelated(ecg_signals).iloc[0, 0]\n",
    "                df_hr.iloc[0, i] = hr\n",
    "                i += 1\n",
    "        bn_col += 1\n",
    "    \n",
    "    if bn_col == 1:\n",
    "        for marker in markers:\n",
    "            curr_idx = df_sub.index[df_sub['marker'] == marker]\n",
    "            if marker == 201:\n",
    "                for y in range(0, 300000, 60000): #similarly for hrv\n",
    "                    ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]+y:curr_idx[0]+y+60000], sampling_rate=1000)\n",
    "                    hrv = nk.ecg_intervalrelated(ecg_signals).iloc[0, 9]\n",
    "                    df_hr.iloc[0, i] = hrv\n",
    "                    i += 1\n",
    "            else:\n",
    "                ecg_signals, info = nk.ecg_process(df_sub[\"ECG\"].iloc[curr_idx[0]:curr_idx[0]+60000], sampling_rate=1000)\n",
    "                hrv = nk.ecg_intervalrelated(ecg_signals).iloc[0, 9]\n",
    "                df_hr.iloc[0, i] = hrv\n",
    "                i += 1\n",
    "    \n",
    "    df_hr.index = [filename.replace('.csv', '')] #saving to df with the subject's id as the index name\n",
    "    return df_hr\n",
    "\n",
    "def process_others_s2(df, file_path, bunch_of_cols, filename):\n",
    "    '''\n",
    "    Function to process blood pressure channels together with accelerometers data and merge with the existing DataFrame.\n",
    "\n",
    "    Input:\n",
    "    df (pd.DataFrame): The existing DataFrame with calculated HR and HRV means to merge new data into.\n",
    "    file_path (str): The path to the CSV file containing the data.\n",
    "    bunch_of_cols (list of lists of str): A list of lists where each sublist contains column names for the different physiological metrics.\n",
    "    filename (str): The name of the file to be used as the index for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with the processed data merged into the input DataFrame `df`.\n",
    "                  The index is based on the provided filename, and the columns are defined by the input `bunch_of_cols`.\n",
    "    '''\n",
    "    i = 0\n",
    "    bn_col = 0\n",
    "    df_sub = pd.read_csv(file_path, usecols=['SBP', 'DBP', 'CO', 'TPR', 'wr', 'tl', 'tr', 'marker'])\n",
    "    markers = [201,211,212,213,214,215,216,221,222,223,224,225,226,231,232,233,234,235,236,241,242,243,244,245,246,251,252,253,254,255,256,261,262,263,264,265,266,271,272,273,274,275,276,281,282,\n",
    "               283,284,285,286]\n",
    "    dataframes = []\n",
    "    for set_cols in bunch_of_cols[2:]: #selecting sets of columns excluding hr and hrv\n",
    "        df_temp = pd.DataFrame(columns=set_cols)\n",
    "        df_temp.loc[0] = [None] * len(set_cols)\n",
    "        for marker in markers:\n",
    "            curr_idx = df_sub.index[df_sub['marker'] == marker]\n",
    "            if marker == 201:\n",
    "                for y in range(0, 300000, 60000):\n",
    "                    df_sub_means = df_sub.iloc[curr_idx[0]+y:curr_idx[0]+60000+y, :-1].mean().iloc[bn_col] #calculating the average for minute fragments of the baseline\n",
    "                    df_temp.iloc[0, i] = df_sub_means #incorporating data into df in the correct place\n",
    "                    i += 1 #moving to the next column \n",
    "            else:\n",
    "                \n",
    "                # print(df_sub.iloc[curr_idx[0]+y:curr_idx[0]+60000+y, :-1].mean())\n",
    "                df_sub_means = df_sub.iloc[curr_idx[0]:curr_idx[0]+60000, :-1].mean().iloc[bn_col]\n",
    "                df_temp.iloc[0, i] = df_sub_means\n",
    "                i += 1\n",
    "        dataframes.append(df_temp) #collecting dataframes into a list\n",
    "        bn_col += 1\n",
    "        i = 0\n",
    "    \n",
    "    df_final = pd.concat(dataframes, axis=1) #merging df with calculated hr and hrv with the list of calculated dfs from the averages of blood pressure and accelerometers\n",
    "    df_final.index = [filename.replace('.csv', '')]\n",
    "    \n",
    "   \n",
    "    \n",
    "    df_merged = df.join(df_final, how='outer')\n",
    "    return df_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc64407-d4e9-4f7e-9d6c-71f907605f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ECG_Rate_mean', 'HRV_RMSSD', 'SBP', 'DBP', 'CO', 'TPR', 'wr', 'tl', 'tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d71f7-abea-4155-bf3b-bedf891afb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_channels = ['HR', 'HRV', 'SBP', 'DBP', 'CO', 'TPR', 'wr', 'tl' ,'tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e332b48-cd8a-4b3e-a2f1-7b50c37a6c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the column template for channels \n",
    "cols_template = [\"baseline_visit2_min1_[channel]\", \"baseline_visit2_min2_[channel]\", \"baseline_visit2_min3_[channel]\", \"baseline_visit2_min4_[channel]\", \"baseline_visit2_min5_[channel]\", \"tournament1_baseline_min1_[channel]\",\n",
    "\"tournament1_baseline_min2_[channel]\", \"tournament1_gameplay_min1_[channel]\", \"tournament1_gameplay_min2_[channel]\", \"tournament1_recovery_min1_[channel]\", \"tournament1_recovery_min2_[channel]\", \"tournament2_baseline_min1_[channel]\",\n",
    "\"tournament2_baseline_min2_[channel]\", \"tournament2_gameplay_min1_[channel]\", \"tournament2_gameplay_min2_[channel]\", \"tournament2_recovery_min1_[channel]\", \"tournament2_recovery_min2_[channel]\", \"tournament3_baseline_min1_[channel]\",\n",
    "\"tournament3_baseline_min2_[channel]\", \"tournament3_gameplay_min1_[channel]\", \"tournament3_gameplay_min2_[channel]\", \"tournament3_recovery_min1_[channel]\", \"tournament3_recovery_min2_[channel]\", \"tournament4_baseline_min1_[channel]\",\n",
    "\"tournament4_baseline_min2_[channel]\", \"tournament4_gameplay_min1_[channel]\", \"tournament4_gameplay_min2_[channel]\", \"tournament4_recovery_min1_[channel]\", \"tournament4_recovery_min2_[channel]\", \"tournament5_baseline_min1_[channel]\",\n",
    "\"tournament5_baseline_min2_[channel]\", \"tournament5_gameplay_min1_[channel]\", \"tournament5_gameplay_min2_[channel]\", \"tournament5_recovery_min1_[channel]\", \"tournament5_recovery_min2_[channel]\", \"tournament6_baseline_min1_[channel]\",\n",
    "\"tournament6_baseline_min2_[channel]\", \"tournament6_gameplay_min1_[channel]\", \"tournament6_gameplay_min2_[channel]\", \"tournament6_recovery_min1_[channel]\", \"tournament6_recovery_min2_[channel]\", \"tournament7_baseline_min1_[channel]\",\n",
    "\"tournament7_baseline_min2_[channel]\", \"tournament7_gameplay_min1_[channel]\", \"tournament7_gameplay_min2_[channel]\", \"tournament7_recovery_min1_[channel]\", \"tournament7_recovery_min2_[channel]\", \"tournament8_baseline_min1_[channel]\",\n",
    "\"tournament8_baseline_min2_[channel]\", \"tournament8_gameplay_min1_[channel]\", \"tournament8_gameplay_min2_[channel]\", \"tournament8_recovery_min1_[channel]\", \"tournament8_recovery_min2_[channel]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63a7132-453a-438d-a473-7c623cf545e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_cols = replace_channel(cols_template, list_of_channels[0])\n",
    "hrv_cols = replace_channel(cols_template, list_of_channels[1])\n",
    "sbp_cols = replace_channel(cols_template, list_of_channels[2])\n",
    "dbp_cols = replace_channel(cols_template, list_of_channels[3])\n",
    "co_cols = replace_channel(cols_template, list_of_channels[4])\n",
    "tpr_cols = replace_channel(cols_template, list_of_channels[5])\n",
    "wr_cols = replace_channel(cols_template, list_of_channels[6])\n",
    "tl_cols = replace_channel(cols_template, list_of_channels[7])\n",
    "tr_cols = replace_channel(cols_template, list_of_channels[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9034f-32de-497d-a4ee-f6ade84aed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "bunch = [hr_cols, hrv_cols, sbp_cols, dbp_cols, co_cols, tpr_cols, wr_cols, tl_cols, tr_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e7f952-13d6-4418-9f9a-56369006e67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491df57-d403-4b15-be6e-dbc1a1d6b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing all CSV files in the folder\n",
    "input_folder = '/home/ubuntu/eSportData/jupyter-data/VU_AMS/s2_output/'\n",
    "output_df = pd.DataFrame()\n",
    "\n",
    "for file_name in os.listdir(input_folder):  #iterating through the list with the databases of the subjects\n",
    "    if file_name.endswith('.csv'):\n",
    "        try:\n",
    "            print(file_name)\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            processed_df = process_hr_hrv_s2(file_path, bunch, file_name) #calling the function to create df with calculated hr and hrv averages\n",
    "            final_df = process_others_s2(processed_df, file_path, bunch, file_name) #calling the function to calculate blood pressure and accelerometer averages and merge them with hr and hrv into a common df\n",
    "            output_df = pd.concat([output_df, final_df]) #adding the subject's data from the iteration to the collective df with other subjects\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# Saving the resulting DataFrame to a CSV file\n",
    "output_df.to_csv('/home/ubuntu/eSportData/jupyter-data/VU_AMS/processed_means_s2_v2_patched.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eec902-43fd-4342-a397-d5399755bccd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
